{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irsUP3YnE-Ra"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjhHeswnCaGM",
    "outputId": "f37c294e-ee85-4456-ed35-4b8def09be1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Columns in dataset: Index(['Unnamed: 0', 'type', 'review', 'label', 'file'], dtype='object')\n",
      "                                              review  \\\n",
      "0  Once again Mr. Costner has dragged out a movie...   \n",
      "1  This is an example of why the majority of acti...   \n",
      "2  First of all I hate those moronic rappers, who...   \n",
      "3  Not even the Beatles could write songs everyon...   \n",
      "4  Brass pictures (movies is not a fitting word f...   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0  mr costner dragged movie far longer necessary ...  \n",
      "1  example majority action film generic boring re...  \n",
      "2  first hate moronic rapper could nt act gun pre...  \n",
      "3  even beatles could write song everyone liked a...  \n",
      "4  brass picture movie fitting word really somewh...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    df = pd.read_csv(\"/content/imdb_master.csv\", encoding=\"ISO-8859-1\", on_bad_lines='skip')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "    df = None  # Set df to None to avoid further errors if loading fails\n",
    "\n",
    "# Proceed if df is loaded\n",
    "if df is not None:\n",
    "    # Check column names\n",
    "    print(\"Columns in dataset:\", df.columns)\n",
    "\n",
    "    # Verify 'review' column exists and drop rows with NaN values in the review column\n",
    "    if 'review' in df.columns:\n",
    "        df = df.dropna(subset=['review'])\n",
    "    else:\n",
    "        print(\"No 'review' column found in the dataset.\")\n",
    "        raise ValueError(\"Expected 'review' column not found\")\n",
    "\n",
    "    # Initialize the lemmatizer and stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Define the text preprocessing function\n",
    "    def preprocess(text):\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Tokenize the text\n",
    "        words = word_tokenize(text)\n",
    "        # Remove stopwords and lemmatize\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "        return \" \".join(words)\n",
    "\n",
    "    # Apply the preprocessing function to the review column\n",
    "    df['cleaned_review'] = df['review'].apply(preprocess)\n",
    "\n",
    "    # Display a few samples of the cleaned data\n",
    "    print(df[['review', 'cleaned_review']].head())\n",
    "else:\n",
    "    print(\"Data loading failed. Please check the file path or format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drDwds0TD-AA"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = [review.split() for review in df['cleaned_review']]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovRZL02eD_5V"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Bag of Words\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "bow_matrix = vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# One-Hot Encoding (with binary=True)\n",
    "one_hot_matrix = CountVectorizer(max_features=1000, binary=True)\n",
    "one_hot_encoding = one_hot_matrix.fit_transform(df['cleaned_review'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55S2VxjW1Sab"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRBg-K7jEG8P"
   },
   "source": [
    "**Perform any two of the basic NLP tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smBOuLERETwy",
    "outputId": "0ca114d6-c252-43e9-eae9-883c556dcda6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import download\n",
    "\n",
    "# Download VADER lexicon\n",
    "download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate sentiment\n",
    "df['sentiment_score'] = df['cleaned_review'].apply(lambda review: sid.polarity_scores(review)['compound'])\n",
    "df['sentiment'] = df['sentiment_score'].apply(lambda score: 'positive' if score > 0 else 'negative')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-dLmoaXE2dx"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # Load the spaCy English language model\n",
    "df['named_entities'] = df['cleaned_review'].apply(lambda text: [(ent.text, ent.label_) for ent in nlp(text).ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbSDehV0mS6T"
   },
   "source": [
    "Link to the GITHUB https://github.com/Haris-Khan7/NLP-AUST-1\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
